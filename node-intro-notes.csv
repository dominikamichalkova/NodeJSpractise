Note,,Module,Clip,
"server-side JS platform; at the high level Node is comprised of 3 building blocks - libuv - a hig performance, cross-latform evented IO library; V8 Googles JS engine also used in Chrome - to leverage V8 OOB within Node - makes it easier for the team to include updated versions of V8 in each release of Node and tereby benefit form Googles continuous innovation of their JS engine; custom C++ and JS code developed specifically for Node platform itself ",,Getting Started with Node.js,Node.js Background,
Node REPL read eval print look - command promt,,Getting Started with Node.js,Demo: Installing Node on Linux with NVM,
"event loop - in the browser, event loop is constatnly listening for DOM events (key press, mouse click etc), node's event loop is constantly listening on the server side",,Getting Started with Node.js,Node's Event Loop,
"those events can be externally generated, eg. incoming http req or tcp connections or timers and other internal events generated by node app itself; additionally other events may be triggered on the response to a request against an external resource - eg. asking node to open a file for reading willfire an event when the file is opened and ready; sending a msg to an external process will fire an event when the msg has been setn; making a req to network resource - eg. another web server, will fire an event when http response is received  --> point: each of these are handled as discrete events in node; in fact the events will likely interleave each other; eg. timer event is received between the re for a file and when the file is ready for reading and both tcp and http events are received while we are sending a msg to an external process. Node itselft does not pause and wait for any of these requests to complete, it continues to react to events as they arrive, a comon example to demonstrate this non-blocking event-driven approach is a web app tat fetches data from a DB - te app raises an event when an http req is received, this event generates a query to DB for some info; once node receives an event back form DB that the query is complete, http response is formulated and sent to the caller; while waiting for the response form DB, node is not blocked and is free to handle additional request (#2,#3..), it can receive a 2nd request while still waiting for the first one to complete etc - they overlapping - it is fundamental to manage multiple threads to achive this type of concurrency, a code in this async enviro requires a different way of thinking",,Getting Started with Node.js,Node's Event Loop,
"compared to JS - each function returns a value before the next funciton is called, each statement builds on the results of the prior one - connect to DB - use that connection to create a statement, from that statement we execute a query and get back set of results and we iterate over these results; NodeJS - like nested approach - closing brackets on the EOF -> find a way to convey the ordering of our statements - get a connection to DB, once you have it, call this func and pass it the connection you just created - Node is left to do other work while it is waiting for DB connectin establishing; callbacks to write code that will run async; eventemitter - capable of emitting events in the future, when each row of the query result becomes available - to tell Node to invoke a func wen each row event is omitted by the result object",,Getting Started with Node.js,Node Conventions for Writing Asychronous Code,
"function(regular input parameter, call-back funtion to invoke once the get stuff function has completed{}) ",,Getting Started with Node.js,Node Conventions for Writing Asychronous Code,
"callback - last parameter in async function call, error - first parameter passed to the callback shoudl always be an error parameter;  it is convenient to verify if a function succeeded by checking for and or false-E error value of callback; handleResults later passed as a callback to GetStuff, it is viable, many functions by the end that are used only once incode",,Getting Started with Node.js,Node Conventions for Writing Asychronous Code,
"for simple callbacks or those that are only referenced once it is very common to use anonymous functions as callbacks, the anonym func is defined within the parameter list of the calling func; cascade down a series of functions ",,Getting Started with Node.js,Node Conventions for Writing Asychronous Code,
"modules can export variables foo.alpha, incl.functions foo.beta(), objects var bar = new Bar()",,"Modules, require() and NPM","Introduction, Accessing Built-in Modules",
"built-in pre-packaged modules in NodeJS - setTimeout, setInterval; other 3rd party modules NPM registry must be installed and then explicitly included - require()",,"Modules, require() and NPM","Introduction, Accessing Built-in Modules",
"require - to also access functionality located within other files of the project, we can look at each JS file as its own module and can expose functionality to be required by other files - to modularize the code, easier to develop and maintain; required files with file system-like semantics, require a file in the same directory,sub-/, ...  or to require just a single variable form another JS script (/...).x.js",,"Modules, require() and NPM",Using require() to Modularize Your Application,
variables are marked fo export via module.exports,,"Modules, require() and NPM",Using require() to Modularize Your Application,
callbacks - way to implement async non-blocking code; another way through Events,,Events and Streams,Events and the EventEmitter class,
"getThem func return value immediately, value is an instance of the EventEmmiter class; results object has an .on function - specify for each item event execute this function passing in the current item; then on the done event or when there are no more results, invoke a new function and if there is an error, invoke a func a pass in the err that occurred",,Events and Streams,Events and the EventEmitter class,
"callback model - req/res; make a req and provide a func to be called wen the req is completed; no results to receive until all the results are received - callback will not be invoked until the entire items array is ready, until the last item has arrived, getThem func will be storing the entire list of items in memory while accumulating them prior to invoking the callback with the entire array; an all or nothing proposition - either error or results; event model - publish/subscribe; invoke .on func repeatedly to provide multiple func to invoke on each event; in essence subscribing to the events; act on results as they arrive - functions associated with the item event will be invoked for each item, opportunity to act on the 1st item as soon as it arrives, 2nd etc, getThem func is not accumulating the items in memory; an error is emitted as a separate event - item and don events do not pass in an error parameter as the 1st value, err can be emitted instead of any item events or after some item events have already been omitted - access to partial results before error",,Events and Streams,Events and the EventEmitter class,
"EventEmitter class - a construct for building these event-driven interfaces; SUBSCRIBER: emitter.on(event,listener) - subscribing events, .on func of the eventemitter instance and specify the event being subscribed to; PUBLISHER: emitter.emit(event,[args]) - publishing events will call the emit func and specify the event being emitted; the event can be any string; event can be emitted with 0 or more arguments, these will pass as parameters to any functions subscribed to that event; included item itself that was passed to the item event as well as the err object passed to the err event; set of events and their arguments constitute an interface exposed to the subscriber by the publiser (emitter)",,Events and Streams,Events and the EventEmitter class,
"2 common patterns for eeventEmitters: 1. as a return value from a func call, instance of eventemitter is created directly and returned from a func 2. objects that extend eventemitter to emit events themselves",,Events and Streams,Events and the EventEmitter class,
"stream - extends the EventEmitter class; streams are instances of extensions to eventemitter with and implementted an agreed upon interface - set of events and other function; these events provide an unified abstraction for dealing with multiple types of dataflow incl. network traffic: http requests/responses, tcp sockets, file I/O, stdin/stdout/stderr,...each stream is an instance of either ReadableStream - sth you would read from or WritableStream - to write to or both. ReadableStream can be pipe()'d to a writeablestream - data from read is piped to write, node handles the backpressure and to address the scenario where a readablesream provides data faster than a writeablestream can consume it",,Events and Streams,"Readable and Writable Streams, the Pipe function",
"READABLESTREAM - its interface includes a Boolean indicating whether the stream is currently readable or not; a series of events that are emitted when new data arrives or when there is no more data= event:'data','end','error','close'; series of func to pause(), resume(),destroy(),pipe() the stream",,Events and Streams,"Readable and Writable Streams, the Pipe function",
"WRITEABLESTREAM interface includes similar Bool indicator writable [boolean]; events that are emitted such as drain when it is safe to write to this stream, pipe when this stream has been passed to a readablestream's pipe function, drain,error,close,pipe; functions: write(),end(),destroy(),destroySoon()",,Events and Streams,"Readable and Writable Streams, the Pipe function",
"when we invoke pipe function on readablestream, we pass as a parameter the writeablestream we want to pipe to, thi sin turn emits the pipe event on the writeable stream; the pipe func then begins an orchestration of events and functions between the 2 streams. when data arrives to the readablestream, data event is emitted and the write() on the writeable stream is invoked with this data; if at some point te write() returns a false value indicating no more data should be written, the pause() of the readablestream is called to stop the flow of data. then once the WS is ready to receive more data, the drain event is emitted and te resume() on the RS is invoked. Once RS is finished, the end event is emitted and the end() on the WS is invoked -- works accross network, file, process communication",,Events and Streams,"Readable and Writable Streams, the Pipe function",
"process object provides a way for out node app to both manage its own process + other processes on our system; available by default - no need to be required; contains a variety of variables and functions incl. a set of streams for accessing process.stdin, stdout, stderr. The first is a RS and te latter 2 are WS; it also provides a series of attributes about the current process such as its set of env variables process.env, CL arguments process.argv, process id (pid) and title, uptime(),memoryUsage(), cwd(); it provides a set of functions process.abort(),chdir().kill(),setgid(),seruid() act on the current running process; process obj is an instance of the eventEmitter class, it emits and exits event when the process is about to exit; it can also emit an uncaught exception event if an exception bubbless all the way up to the event loop",,Accessing the Local System,"Introduction, The Process Object",
"interacting with the file system through built in ,,fs"" module, many of the func provided by this moduel are wrappers around  the POSIX functions(both async and syn versions) incl. rename, truncate, chon, fchown, lchown, chmod, fchmod, lchmod,stat,fstat,lstat,link,symlink,readlink,realpath,unlink,rmdir,mkdir,readdir,close,open,utimes,futimes,fsync,write,read,readFile,writeFile,appendFile,e.g. fs.readdir(path,callback)",,Accessing the Local System,Interacting with the File System,
"the fs provides also couple of stream oriented funcions: fs.createReadStream() - opens a file for reading and returns an fs.ReadStream(a RS), fs.createWriteStream - returns an fs.WriteStream(a WS); useful for integrating with other streams; fs.watch() - watch a file or directory for changes - returns an eventEmitter which emits change event whenever a file changes fs.FSWatcher(an EventEmitter)",,Accessing the Local System,Interacting with the File System,
"invoke .toString()  on the value returned wen we read a file from FS-> caused by return value for this function was a buffer object. JS has difficulty to deal with binary data, but interacting with the network and FS require it. Buffer class provides a raw memory allocation for dealing with binary data directly. Bufferes can be converted to/from strings by providing an encoding: utf8(default),ascii,utf16le, base64, binary,hex. When we call .toString on our buffer returned fro mreading a file we were usiing the default encoding to convert it to a string. This support for multiple encodings makes buffers a handy way ot convert strings to and from base64",,Accessing the Local System,What is a Buffer?,
"os module - functions for examing the OS the NodeJS app is running on and to interact with the local environment - os. tmpDir(),hostname,type,platform, arch, release, freemem,totalmem,loadavg,uptime,cpus,networkInterfaces",,Accessing the Local System,"The OS Module, Conclusion",
"making web requests - require http var re = http.request(options,function(res){//process callback}); takes 2 parameters; options can be: 1.an URL string 2. an object specifying values for host, port, method,path, headers,auth.. The request function returns a value, which is an instance of client request http.ClientRequest(a WS) and can be written or piped to for HTTP post uploads; request function also takes a callback parameter, wen invoked is passed a single parameter and instance of http.ClientResponse(a RS) which represents the results of the HTTP request - provided via callback or as a response event on the request object; can be read from or piped to a WS; http.get() available as a simplified interface for EGT requests",,Interacting with the Web,"Introduction, Making Web Requests in Node",
http client requests,,Interacting with the Web,Demo: Making Web Requests in Node,
"var server = http.createServer(function(req,res){//process req});server.listen(port,[host]); each req is provided viac either callback or as a request event on the server object. Callback to be invoked each time a req is received by the web server; optionally if no callback is provided, req ca also be received by listening for events on the server object that is returned. Server begins accepting http request after the listen() is called; when we made a req to http server and th ecallback is involved, it is passed 2 parameters - 1.instance of server req (RS) - instance of http.ServerRequest 2. 2nd parameter passd to the callback on each web req is a server response object (WS) - instance of http.ServerResponse - the response sent to the client. If we return stream-oriented data, such as a file from disk, we can pipe that stream to the server response WS; Support for SSL requests are addressed by a separate HTTPS module",,Interacting with the Web,Building a Web Server in Node,
"socket.io provides an abstraction over the various methods used to maintain an active connection between a browser and a server; it will use web sockets where they are supported and will transparently fall back to several other techniques in case where web sockets are not yet supported, either due to browser or firewall limitations; in the case of node.js it also provieds a consistent interface for performing these socket-based communications in both the browser and the server - demenstration between JS on the client and the server; on the browser side, socket.io JS library is loaded from the server",,Interacting with the Web,Realtime Interaction with Socket.IO,
"On the browser side, the Socket.IO JavaScript library is loaded from the server. There is no special configuration on the server to provide this JavaScript file. That is handled transparently by the Socket.IO Node.js module.  The browser will issue IO.connect to establish a connection to the Node.js server. The server receives a connection event and emits a news event with a payload, ""Hello world."" The browser receives this news event and invokes the appropriate function. Within this function, the browser emits an event entitled, ""my other event,"" and provides some data. This event is received on the Node.js server and the appropriate function is invoked which logs the payload data to the console. The powerful thing about this scenario is that both the browser and the server are using the same constructs for emitting and acting on messages being passed back and forth. The code on the server looks very similar to that in the browser and vice-versa. During this type of development in other server site languages it's certainly possible, but it will lack the symmetry and implementation and all the benefits that come with it. ",,Interacting with the Web,Realtime Interaction with Socket.IO,
"criticism of NodeJS apps - not handling CPU intensive tests well; because spending too much time on any one task in NOde app will block the event loop and prevent other work from being done -> one strategy to deal with this issue is a use of child processes, a CPU intensive task, resizing an image e.g. can be deferred to a child process while the main NOde app continues to process events. 4 ways to invoke/launch a process - all are part of the child_process module",,Scaling Your Node Application,"Introduction, The Child Process Module",
"1. spawn (command,[arg],[options]) - launch a new process and run the command specified in the first parameter, an optional array of arg will be passed to the command; returns an instance of a childProcess object which is an EventEmitter and emits exit, close events which can be listened for by the parent Node app; child process return value also has streams for stdin, stdout, stderr  and the parent o spawning process can pipe data to/from these streams ",,Scaling Your Node Application,"Introduction, The Child Process Module",
"2. exec(command,[options],callback) - run command string in a shell, any CL arg must be included in the string passed as the command to execute; even pipe between unix commands within a single invocation of exec, e.g. piping the output of ls to the unix grep command; callback is invoked on process completion with error, stdout, stderr",,Scaling Your Node Application,"Introduction, The Child Process Module",
"3. execFile(file,[arg],[options],callback) - similar to exec() but instead of launching a process and executing the command, the file parammeter is executed directly ather than in a subshell",,Scaling Your Node Application,"Introduction, The Child Process Module",
"4. fork(modulePath,[arg],[options]) - optimized way special version of spawn especially for creating Node processes, adds a send() and message event to ChildProcess;  returns an instance of the ChildProcess object, however in this case it adds an additional send() and message event to facilitate message between the parent a child processes",,Scaling Your Node Application,"Introduction, The Child Process Module",
"parent.js requires child_process module, calls the fork() to invoke Node again an run child.js script; it listens for the message event from the returned ChildProcess object logging the message to the console; then it attempts to send a msg to the newly forked Node process; when the separate node process runs child.js, it listens for the message event and logs the msg to the console; it also attempts to send a msg back to its parent; each process sends its message which is picked up by the other and logged to the console",,Scaling Your Node Application,"Introduction, The Child Process Module",
" Node has recently introduced an experimental cluster module. This builds on the child_process.fork() and introduces classes, functions, and events for managing a master application and a set of ""Worker"" Nodes. In a typical cluster scenario, there will be a Node.js script which serves as the master application. The cluster module provides a variable isMaster, which will tell you if your code is running the master process. To create Worker Nodes, actually separate Node.js processes, Cluster provides a fork(). By default, fork will run the same Node.js script for the worker as the master. You can use the isMaster variable in a large if statement to segment the master application from the worker code. Executing the fork function will also emit a fork event in the master. Once the worker process has been spawned, the master will also emit an online event indicating that the worker has been created. Additional workers can be created with subsequent invocations of the fork function. Cluster also provides an isWorker variable to indicate whether your code is executing inside a worker Node. A common cluster scenario is spawning multiple worker Nodes to create a scalable web server, if the worker executes  listen(), a listening event is emitted on the master. And the arguments to this listen function are transmitted to the master where it will create a listening server for that IP import if one does not already exist, and pass a handle to this server back to the worker. If another worker process executes the same listen function, the master will send the same handle to this second worker. This allows both workers to listen on the same IP port combination. It is then up to the operating system as to how incoming requests are distributed between worker processes. They do not proxy through the master. Requests are sent directly to worker processes. Let's take a look at an example of using the cluster module to set up a multi worker web server.",,Scaling Your Node Application,Scaling with Node's Cluster Module,
